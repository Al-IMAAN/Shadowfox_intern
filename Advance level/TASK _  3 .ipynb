{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3304f0e6-133c-41fb-91d6-fb7a61f1cfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "from requests.exceptions import HTTPError, ConnectionError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19594baf-b890-4d25-be34-1cf29906fdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up your OpenAI API key\n",
    "openai.api_key = \" \" # actual API key\n",
    "\n",
    "def generate_response(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a response from GPT-3 based on the provided prompt.\n",
    "    \n",
    "    Parameters:\n",
    "        prompt (str): The prompt to send to GPT-3.\n",
    "    \n",
    "    Returns:\n",
    "        str: The response generated by GPT-3 or an empty string in case of an error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = openai.Completion.create(\n",
    "            engine=\"text-davinci-003\",  # GPT-3 model used\n",
    "            prompt=prompt,\n",
    "            max_tokens=150\n",
    "        )\n",
    "        return response.choices[0].text.strip()\n",
    "    except (HTTPError, ConnectionError) as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a291197f-286d-4a34-b64c-cbde793fdc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_coherence(response: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check the coherence of the response by looking for common logical connectors.\n",
    "    \n",
    "    Parameters:\n",
    "        response (str): The response text from GPT-3.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if the response contains logical connectors, False otherwise.\n",
    "    \"\"\"\n",
    "    connectors = [\"however\", \"therefore\", \"consequently\", \"moreover\", \"thus\"]\n",
    "    return any(connector in response.lower() for connector in connectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9c50892-272c-4fcd-be6e-38e35898f3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_consistency(prompt: str, response: str) -> float:\n",
    "    \"\"\"\n",
    "    Check the consistency of the response with the prompt by comparing keywords.\n",
    "    \n",
    "    Parameters:\n",
    "        prompt (str): The original prompt.\n",
    "        response (str): The response text from GPT-3.\n",
    "    \n",
    "    Returns:\n",
    "        float: A score representing the consistency of the response with the prompt.\n",
    "    \"\"\"\n",
    "    prompt_keywords = [word.lower() for word in prompt.split()]\n",
    "    response_keywords = [word.lower() for word in response.split()]\n",
    "    common_keywords = set(prompt_keywords) & set(response_keywords)\n",
    "    return len(common_keywords) / len(prompt_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0114f95-f134-4538-a681-1a0a3b015df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_prompts(prompts: List[str]) -> Tuple[List[str], List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Generate responses and analyze each prompt for coherence and consistency.\n",
    "    \n",
    "    Parameters:\n",
    "        prompts (List[str]): A list of prompts to be analyzed.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[List[str], List[float], List[float]]:\n",
    "            - responses (List[str]): The responses generated for each prompt.\n",
    "            - coherence_scores (List[float]): Coherence scores for each response.\n",
    "            - consistency_scores (List[float]): Consistency scores for each response.\n",
    "    \"\"\"\n",
    "    responses = [generate_response(prompt) for prompt in prompts]\n",
    "    coherence_scores = [check_coherence(response) for response in responses]\n",
    "    consistency_scores = [check_consistency(prompt, response) for prompt, response in zip(prompts, responses)]\n",
    "    return responses, coherence_scores, consistency_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "febe1c8a-71a8-4bb5-9ee0-76f19a2e85eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_scores(prompts: List[str], coherence_scores: List[float], consistency_scores: List[float]) -> None:\n",
    "    \"\"\"\n",
    "    Visualize coherence and consistency scores using bar plots.\n",
    "    \n",
    "    Parameters:\n",
    "        prompts (List[str]): The prompts analyzed.\n",
    "        coherence_scores (List[float]): Coherence scores for each prompt.\n",
    "        consistency_scores (List[float]): Consistency scores for each prompt.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a DataFrame for visualization\n",
    "        data = {\n",
    "            'Prompt': prompts,\n",
    "            'Coherence Score': coherence_scores,\n",
    "            'Consistency Score': consistency_scores\n",
    "        }\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # Visualization of coherence scores\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(x='Prompt', y='Coherence Score', data=df)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.title('Coherence Scores for Different Prompts')\n",
    "        plt.tight_layout()  # Adjust layout to fit labels and titles\n",
    "        plt.show()\n",
    "\n",
    "        # Visualization of consistency scores\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(x='Prompt', y='Consistency Score', data=df)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.title('Consistency Scores for Different Prompts')\n",
    "        plt.tight_layout()  # Adjust layout to fit labels and titles\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during visualization: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
